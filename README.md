# LocaLLM

Un mini-ChatGPT qui fonctionne directement dans votre navigateur, sans cloud ni collecte de donn√©es.

![LocaLLM Demo](public/demo/locallm.gif)

## üöÄ **Essayez LocaLLM maintenant !**

<div align="center">

### [**üåê D√©mo en ligne : chat.hugosoft.dev**](https://chat.hugosoft.dev)

*‚ú® Aucune installation requise - Testez imm√©diatement dans votre navigateur*

[![Try Demo](https://img.shields.io/badge/üöÄ-Essayer_la_d√©mo-007ACC?style=for-the-badge&logo=play&logoColor=white)](https://chat.hugosoft.dev)

</div>

---

## Fonctionnalit√©s principales
- **üß† Ex√©cution locale** : Chargement local de LLM open source gr√¢ce √† **WebLLM**
- **üí¨ Interface moderne** : Chat avec s√©lection du mod√®le et interface intuitive
- **‚ö° Streaming en temps r√©el** : R√©ponses g√©n√©r√©es en streaming pour un rendu fluide
- **üìù Support Markdown** : Rendu complet du markdown avec **react-markdown** et **remark-gfm**
- **‚úèÔ∏è √âdition de messages** : Possibilit√© de modifier et r√©g√©n√©rer les messages utilisateur
- **üíæ Historique persistant** : Sauvegarde automatique des conversations localement dans **IndexDB**
- **üîç Recherche intelligente** : Recherche dans les titres et contenus des conversations

## Installation rapide
```bash
npm install
npm run dev
```
Ouvrez ensuite votre navigateur sur [http://localhost:3000](http://localhost:3000).

## üéØ Tutoriel : Choisir votre mod√®le

### Premi√®re utilisation
1. **Ouvrez LocaLLM** dans votre navigateur
2. **La modal d'accueil s'affiche** automatiquement si aucun mod√®le n'est install√©
3. **Choisissez votre mod√®le** dans la liste ci-dessous selon vos besoins
4. **Cliquez sur "T√©l√©charger"** pour installer le mod√®le localement
5. **Attendez le t√©l√©chargement** (peut prendre quelques minutes selon votre connexion)

### Comparaison des mod√®les disponibles

| Mod√®le | Param√®tres | Taille | Score | Recommandation | Utilisation id√©ale |
|--------|------------|--------|-------|----------------|-------------------|
| **Mistral 7B Instruct** | 7B | 4.8GB | A+ | **Recommand√©** | Chat g√©n√©ral, raisonnement, code |
| **Gemma 2 9B** | 9B | 5.0GB | A | PC puissant | T√¢ches complexes, analyse avanc√©e |
| **Llama 3.2 3B** | 3B | 1.5GB | A+ | PC basique | Usage l√©ger, r√©ponses rapides |

### Guide de s√©lection selon votre PC

#### üíª **PC basique (4-8GB RAM, Pas de GPU)**
- **Mod√®le recommand√© :** Llama 3.2 3B
- **Avantages :** Tr√®s rapide, peu de m√©moire, excellent pour l'usage quotidien
- **Limitations :** R√©ponses plus courtes, moins de nuance

#### üñ•Ô∏è **PC standard (8-16GB RAM, GPU r√©cent)**
- **Mod√®le recommand√© :** Mistral 7B Instruct
- **Avantages :** Excellent √©quilibre performance/qualit√©, tr√®s polyvalent
- **Utilisation :** Chat, code, analyse, r√©daction

#### üöÄ **PC puissant (16GB+ RAM, GPU d√©di√©)**
- **Mod√®le recommand√© :** Gemma 2 9B
- **Avantages :** Qualit√© maximale, capacit√©s avanc√©es
- **Utilisation :** T√¢ches complexes, analyse approfondie, g√©n√©ration cr√©ative

### Conseils d'utilisation
- **Commencez par Mistral 7B** si vous h√©sitez - c'est le meilleur compromis
- **Testez plusieurs mod√®les** pour trouver celui qui vous convient
- **Les mod√®les restent install√©s** localement pour un acc√®s rapide
- **Vous pouvez changer de mod√®le** √† tout moment via le s√©lecteur en haut

## Technologies utilis√©es
- [Next.js](https://nextjs.org/) - Framework React full-stack
- [WebLLM](https://mlc.ai/web-llm/) - Ex√©cution locale des LLMs
- [React](https://react.dev/) et [TypeScript](https://www.typescriptlang.org/) - Interface moderne
- [Tailwind CSS](https://tailwindcss.com/) - Styling utilitaire
- [React Markdown](https://github.com/remarkjs/react-markdown) - Rendu markdown
- [IndexDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API) - Stockage local
- [Zustand](https://zustand-demo.pmnd.rs/) - Gestion d'√©tat
- [Radix UI](https://www.radix-ui.com/) - Composants accessibles

## √Ä propos
Ce projet est fourni sous licence MIT. Il permet d'exp√©rimenter facilement les mod√®les de langage directement sur sa machine, avec une exp√©rience utilisateur moderne et compl√®te.

## Docker
Pour lancer l'application dans un conteneur :
```bash
docker build -t locallm .
docker run -p 3000:3000 locallm
```
La configuration s'effectue via des variables d'environnement pass√©es au conteneur.

## Docker Compose
Vous pouvez √©galement d√©marrer le service via `docker compose` :
```bash
cp .env.example .env # cr√©ez vos variables d'environnement
docker compose up --build
```

## Contribution
Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue ou une pull request.
